env:
  name: breakout
  is_atari: True
  num_envs: 32
  max_episode_steps: 900   # 30 fps
  stack_stamples: 4
  # env_args:



algo:
  name: ddqn
  gamma: 0.99
  lr_start: 2.5e-4       # ddqn paper (pg. 9) uses constant lr=0.00025
  lr_end:   2.5e-4
  lr_warmup_env_steps: 5000
  batch_size: 32       # ddqn paper (pg. 9) uses B=32
  update_every_steps: 1
  use_action_for_steps_train: 1
  use_action_for_steps_eval: 1
  # seed: 42             # Comment out (Default to None) if you want random seeding
  buffer_size: 40000    # ddqn paper (pg. 9) uses 1M for this but train on atari (200M frames)
  buffer_type: priority_experience_replay
  warmup_buffer_size: 5000
  overwrite_target_net_grad_updates: 10000   # Update target net after this many SGD steps, not environment steps. Num env steps would be this * num_envs * update_every_steps


sampler:
  name: epsilon_greedy
  starting_epsilon: 1.0
  ending_epsilon: 0.1
  warmup_steps: 0
  decay_until_step: 1000000


networks:
  q_1:
    network_type: cnn
    network_args:
      input_format: CHW
      cnn_layers:
        - {out_channels: 32, kernel_size: 8, stride: 4}
        - {out_channels: 64, kernel_size: 4, stride: 2}
        - {out_channels: 64, kernel_size: 3, stride: 1}
      cnn_activations: [relu, relu, relu]
      mlp_dims: ["InDims", 512, "OutDims"]
      mlp_activations: [relu, identity]
      mlp_bias: True
      mlp_layer_type: [Linear, Linear]
      mlp_layer_extra_args: [{}, {}]

  q_2:
    network_type: cnn
    network_args:
      input_format: CHW
      cnn_layers:
        - {out_channels: 32, kernel_size: 8, stride: 4}
        - {out_channels: 64, kernel_size: 4, stride: 2}
        - {out_channels: 64, kernel_size: 3, stride: 1}
      cnn_activations: [relu, relu, relu]
      mlp_dims: ["InDims", 512, "OutDims"]
      mlp_activations: [relu, identity]
      mlp_bias: True
      mlp_layer_type: [Linear, Linear]
      mlp_layer_extra_args: [{}, {}]

train:
  total_env_steps: 1000000
  log_interval: 100000
  eval_interval: 100000
  eval_envs: 64
  # threshold_exit_training_on_eval_score: 250
  logging_method: [console]
  console_log_train: True
  save_video_at_end: True
  save_algo_at_end: True
  wandb_project: rl-benchmarking
  wandb_group: breakout_ddqn
  wandb_run: breakout_ddqn_run1


inference:
  inference_only: False
  override_cfg: False     # If true, override the cfg with what is in this
  algo_path: TBU