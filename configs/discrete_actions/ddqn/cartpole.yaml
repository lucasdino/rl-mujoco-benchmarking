env:
  name: CartPole-v1
  num_envs: 16
  max_episode_steps: 500    # 20 seconds max (30 fps)


algo:
  name: ddqn
  gamma: 0.99
  lr_start: 5e-4       # ddqn paper (pg. 9) uses constant lr=0.00025
  lr_end:   1e-4
  lr_warmup_env_steps: 10000
  batch_size: 32       # ddqn paper (pg. 9) uses B=32
  update_every_steps: 1
  use_action_for_steps_train: 1
  use_action_for_steps_eval: 1
  # seed: 42             # Comment out (Default to None) if you want random seeding
  buffer_size: 100000    # ddqn paper (pg. 9) uses 1M for this but train on atari (200M frames)
  buffer_type: priority_experience_replay
  warmup_buffer_size: 5000
  overwrite_target_net_grad_updates: 32   # In terms of 'number grad updates', not samples seen. Num steps would be this * num_envs * update_every_steps


sampler:
  name: epsilon_greedy
  starting_epsilon: 1.0
  ending_epsilon: 0.1
  warmup_steps: 0
  decay_until_step: 150000


networks:
  q_1:
    network_type: mlp
    network_args:
      mlp_dims: ["InDims", 32, 32, "OutDims"]
      mlp_activations: [relu, relu, identity]
      mlp_bias: True
      mlp_layer_type: [Linear, Linear, Linear]
      mlp_layer_extra_args: [{}, {}, {}]

  q_2:
    network_type: mlp
    network_args:
      mlp_dims: ["InDims", 32, 32, "OutDims"]
      mlp_activations: [relu, relu, identity]
      mlp_bias: True
      mlp_layer_type: [Linear, Linear, Linear]
      mlp_layer_extra_args: [{}, {}, {}]


train:
  total_env_steps: 200000
  log_interval: 10000
  eval_interval: 10000
  eval_envs: 128
  threshold_exit_training_on_eval_score: 500
  logging_method: [console]
  console_log_train: False
  wandb_project: rl-benchmarking
  wandb_group: cartpole-v1_ddqn
  wandb_run: cartpole-v1_ddqn_run1
  save_video_at_end: True
  save_algo_at_end: True


inference:
  inference_only: False
  override_cfg: False     # If true, override the cfg with what is in this
  algo_path: saved_data/saved_algos/ddqn/cartpole-v1_ddqn_run1_20260127_160158.pkl