env:
  name: LunarLander-v3
  num_envs: 16
  max_episode_steps: 500   # 30 fps


algo:
  name: dqn
  gamma: 0.99
  lr_start: 2.5e-4       # ddqn paper (pg. 9) uses constant lr=0.00025
  lr_end:   2.5e-4
  lr_warmup_env_steps: 100000
  batch_size: 32       # ddqn paper (pg. 9) uses B=32
  n_step: 8            # n-step TD learning (1 = standard TD)
  update_every_steps: 1
  use_action_for_steps_train: 1
  use_action_for_steps_eval: 1
  seed: 21             # Comment out (Default to None) if you want random seeding
  buffer_size: 500000    # ddqn paper (pg. 9) uses 1M for this but train on atari (200M frames)
  buffer_type: priority_experience_replay
  warmup_buffer_size: 50000
  overwrite_target_net_grad_updates: 1024   # Update target net after this many SGD steps, not environment steps. Num env steps would be this * num_envs * update_every_steps


sampler:
  name: greedy
  # starting_epsilon: 1.0
  # ending_epsilon: 0.05
  # warmup_steps: 0
  # decay_until_step: 800000


networks:
  q_1:
    network_type: mlp
    network_args:
      mlp_dims: ["InDims", 128, 128, "OutDims"]
      mlp_activations: [relu, relu, identity]
      mlp_bias: True
      mlp_layer_type: [Linear, NoisyLinear, NoisyLinear]
      mlp_layer_extra_args: [{}, {sigma0: 0.5}, {sigma0: 0.5}]


train:
  total_env_steps: 1500000
  log_interval: 100000
  eval_interval: 100000
  eval_envs: 128
  # threshold_exit_training_on_eval_score: 250
  logging_method: [console]
  console_log_train: True
  save_video_at_end: False
  save_algo_at_end: False
  # wandb_project: set this in config.py
  wandb_group: lunarlander_dqn
  run_name: lunarlander_dqn_v1
  num_seeds: 1
  save_result: False


inference:
  inference_only: False
  override_cfg: False     # If true, override the cfg with what is in this
  algo_path: TBU